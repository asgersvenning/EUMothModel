{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 24/121 [00:19<00:49,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fasterrcnn_mobilenet_v3_large_320_fpn has no fc or classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 25/121 [00:20<00:44,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fasterrcnn_mobilenet_v3_large_fpn has no fc or classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 26/121 [00:21<00:57,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fasterrcnn_resnet50_fpn has no fc or classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 27/121 [00:22<01:02,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fasterrcnn_resnet50_fpn_v2 has no fc or classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 30/121 [00:24<01:05,  1.39it/s]/home/ucloud/.local/lib/python3.8/site-packages/torchvision/models/googlenet.py:47: FutureWarning: The default weight initialization of GoogleNet will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
      "  warnings.warn(\n",
      " 26%|██▌       | 31/121 [00:24<00:50,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fcos_resnet50_fpn has no fc or classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ucloud/.local/lib/python3.8/site-packages/torchvision/models/inception.py:43: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
      "  warnings.warn(\n",
      " 28%|██▊       | 34/121 [00:25<00:39,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypointrcnn_resnet50_fpn has no fc or classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 35/121 [00:26<00:52,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maskrcnn_resnet50_fpn has no fc or classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 36/121 [00:27<00:58,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maskrcnn_resnet50_fpn_v2 has no fc or classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 46/121 [00:30<00:19,  3.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mvit_v1_b has no fc or classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 47/121 [00:30<00:27,  2.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mvit_v2_s has no fc or classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 61/121 [00:36<00:21,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raft_large has no fc or classifier\n",
      "raft_small has no fc or classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 87/121 [00:56<00:17,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retinanet_resnet50_fpn has no fc or classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▎  | 89/121 [00:57<00:14,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retinanet_resnet50_fpn_v2 has no fc or classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 96/121 [01:00<00:12,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssd300_vgg16 has no fc or classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 97/121 [01:00<00:11,  2.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssdlite320_mobilenet_v3_large has no fc or classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 98/121 [01:02<00:20,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swin3d_b has no fc or classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 99/121 [01:04<00:21,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swin3d_s has no fc or classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 100/121 [01:04<00:17,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swin3d_t has no fc or classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 101/121 [01:05<00:18,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swin_b has no fc or classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 102/121 [01:06<00:16,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swin_s has no fc or classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 103/121 [01:06<00:13,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swin_t has no fc or classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 104/121 [01:08<00:19,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swin_v2_b has no fc or classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 105/121 [01:10<00:19,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swin_v2_s has no fc or classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 106/121 [01:11<00:16,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swin_v2_t has no fc or classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 115/121 [01:24<00:07,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vit_b_16 has no fc or classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 116/121 [01:24<00:05,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vit_b_32 has no fc or classifier\n"
     ]
    }
   ],
   "source": [
    "supported = []\n",
    "unsupported = []\n",
    "\n",
    "for i in tqdm(torchvision.models.list_models()):\n",
    "    model = torchvision.models.get_model(i)\n",
    "    named_groups = list(dict(model.named_children()).keys())\n",
    "\n",
    "    if not any([j in named_groups for j in ['fc', 'classifier']]):\n",
    "        print(f'{i} has no fc or classifier')\n",
    "        unsupported.append(i)\n",
    "    else:\n",
    "        # print(f'{i} has fc or classifier')\n",
    "        supported.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{len(supported)} supported models')\n",
    "print(f'{len(unsupported)} unsupported models: {unsupported}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/15/2023 11:40:27 - INFO - torch.distributed.nn.jit.instantiator -   Created a temporary directory at /tmp/tmpa7lrl2ci\n",
      "11/15/2023 11:40:27 - INFO - torch.distributed.nn.jit.instantiator -   Writing /tmp/tmpa7lrl2ci/_remote_module_non_scriptable.py\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.io import read_image\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "from math import sqrt, ceil\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "\n",
    "import sahi.predict as predict\n",
    "\n",
    "# Import YOLOv5 helper functions\n",
    "os.chdir(\"/home/ucloud/EUMothModel\")\n",
    "from tutils.yolo_helpers import non_max_suppression\n",
    "from tutils.models import HierarchicalClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sahi import ObjectPrediction, BoundingBox\n",
    "from sahi.postprocess.combine import GreedyNMMPostprocess, LSNMSPostprocess\n",
    "from sahi.prediction import visualize_object_predictions, PredictionScore\n",
    "\n",
    "def ObjectPrediction_to_yolov5_format(obj, num_classes):\n",
    "    # Extract bounding box and objectness score\n",
    "    bbox = obj.bbox.to_voc_bbox()  # Convert to [x_min, y_min, x_max, y_max]\n",
    "    x_center = (bbox[0] + bbox[2]) / 2\n",
    "    y_center = (bbox[1] + bbox[3]) / 2\n",
    "    width = bbox[2] - bbox[0]\n",
    "    height = bbox[3] - bbox[1]\n",
    "    objectness = obj.score.value\n",
    "\n",
    "    # One-hot encode the predicted class\n",
    "    class_id = obj.category.id\n",
    "    class_scores = [0] * num_classes\n",
    "    class_scores[class_id] = 1\n",
    "\n",
    "    # Combine into YOLOv5 format\n",
    "    yolov5_prediction = [x_center, y_center, width, height, objectness] + class_scores\n",
    "\n",
    "    return torch.tensor(yolov5_prediction)\n",
    "\n",
    "def convert_to_yolov5_format(object_predictions, num_classes):\n",
    "    \"\"\"\n",
    "    Convert a list of sahi.ObjectPrediction objects to YOLOv5 prediction format.\n",
    "\n",
    "    Args:\n",
    "    - object_predictions (List[sahi.ObjectPrediction]): List of object predictions.\n",
    "    - num_classes (int): Total number of classes.\n",
    "\n",
    "    Returns:\n",
    "    - List[List[float]]: Converted predictions in YOLOv5 format.\n",
    "    \"\"\"\n",
    "    yolov5_predictions = []\n",
    "\n",
    "    for prediction in object_predictions:\n",
    "        yolov5_predictions += [ObjectPrediction_to_yolov5_format(prediction, num_classes)]    \n",
    "\n",
    "    return torch.stack(yolov5_predictions)\n",
    "\n",
    "def convert_to_yolov5_batch_format(yolov5_list):\n",
    "    \"\"\"\n",
    "    Convert a list of YOLOv5 predictions to a batched YOLOv5 prediction.\n",
    "\n",
    "    Args:\n",
    "    - yolov5_list (List[List[float]]): List of YOLOv5 predictions.\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: Batched YOLOv5 prediction.\n",
    "    \"\"\"\n",
    "    return torch.concat(yolov5_list)\n",
    "\n",
    "def yolov5_to_ObjectPrediction(yp):\n",
    "    \"\"\"\n",
    "    Convert a YOLOv5 style output to a list of sahi.ObjectPrediction objects.\n",
    "\n",
    "    Args:\n",
    "    - yp (torch.Tensor): YOLOv5 style output. Torch.Tensor of shape (n_predictions, 5 + num_classes).\n",
    "    \"\"\"\n",
    "\n",
    "    object_predictions = []\n",
    "\n",
    "    for pred in yp:\n",
    "        x, y, w, h, obj = pred[:5]\n",
    "        class_scores = pred[5:]\n",
    "\n",
    "        # Convert to sahi.BoundingBox\n",
    "        x_min = x - w / 2\n",
    "        y_min = y - h / 2\n",
    "        x_max = x + w / 2\n",
    "        y_max = y + h / 2\n",
    "\n",
    "        # Convert to sahi.Category\n",
    "        category_id = class_scores.argmax().item()\n",
    "\n",
    "        this_obj = ObjectPrediction(\n",
    "            bbox=[x_min.item(), y_min.item(), x_max.item(), y_max.item()],\n",
    "            category_id=category_id,\n",
    "            category_name=\"salient_moth\",\n",
    "            score=obj.item()\n",
    "        )\n",
    "\n",
    "        object_predictions += [this_obj]\n",
    "\n",
    "    return object_predictions\n",
    "\n",
    "def plot_yolov5(yp, image, match_threshold=0.5, match_metric=\"IOS\",**kwargs):\n",
    "    object_predictions = yolov5_to_ObjectPrediction(yp)\n",
    "    postprocessor = GreedyNMMPostprocess(match_threshold=match_threshold, match_metric=match_metric)\n",
    "    object_predictions = postprocessor(object_predictions)\n",
    "    image = np.ascontiguousarray(read_image(image).numpy().transpose(1, 2, 0), dtype=np.uint8)\n",
    "\n",
    "    visualize_object_predictions(\n",
    "        object_prediction_list=object_predictions,\n",
    "        image=image,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "def plot_list(l, image, mult=None, slice_sizes=None, conf_threshold=0.75, match_threshold=0.5, match_metric=\"IOS\",**kwargs):\n",
    "    object_predictions = []\n",
    "    if mult is None:\n",
    "        mult = [1] * len(l)\n",
    "    for i, ol in enumerate(l):\n",
    "        tl = ol.object_prediction_list\n",
    "        tl = [op for op in tl if (op.score.value * mult[i]) >= conf_threshold]\n",
    "        if slice_sizes is not None:\n",
    "            tslice_area = slice_sizes[i] ** 2\n",
    "            tl = [op for op in tl if op.bbox.area <= (tslice_area * 0.5)]\n",
    "\n",
    "        object_predictions += tl\n",
    "    postprocessor = LSNMSPostprocess(match_threshold=match_threshold, match_metric=match_metric)\n",
    "    object_predictions = postprocessor(object_predictions)\n",
    "    image = np.ascontiguousarray(read_image(image).numpy().transpose(1, 2, 0), dtype=np.uint8)\n",
    "\n",
    "    visualize_object_predictions(\n",
    "        object_prediction_list=object_predictions,\n",
    "        image=image,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    out = l[0]\n",
    "    out.object_prediction_list = object_predictions\n",
    "    return out\n",
    "\n",
    "def combine_and_nms(l, mult, slice_sizes, conf_threshold, match_metric, match_threshold):\n",
    "    object_predictions = []\n",
    "    if mult is None:\n",
    "        mult = [1] * len(l)\n",
    "    for i, ol in enumerate(l):\n",
    "        tl = ol.object_prediction_list\n",
    "        tl = [op for op in tl if (op.score.value * mult[i]) >= conf_threshold]\n",
    "        if slice_sizes is not None:\n",
    "            tslice_area = slice_sizes[i] ** 2\n",
    "            tl = [op for op in tl if op.bbox.area <= (tslice_area * 0.5)]\n",
    "\n",
    "        object_predictions += tl\n",
    "    postprocessor = LSNMSPostprocess(match_threshold=match_threshold, match_metric=match_metric)\n",
    "    object_predictions = postprocessor(object_predictions)\n",
    "\n",
    "    out = l[0]\n",
    "    out.object_prediction_list = object_predictions\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ucloud/.local/lib/python3.8/site-packages/albumentations/augmentations/dropout/cutout.py:49: FutureWarning: Cutout has been deprecated. Please use CoarseDropout\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import albumentations as A\n",
    "\n",
    "augment = A.Compose([\n",
    "    A.Cutout(num_holes=4, max_h_size=32, max_w_size=32, p=0.5),\n",
    "    A.GaussNoise(var_limit=(0.01, 0.05), p=0.5),\n",
    "    A.Rotate(limit=45, p=1.0, rotate_method=\"ellipse\"),\n",
    "    A.RandomBrightnessContrast(p=0.5),\n",
    "    A.VerticalFlip(p=0.5)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ucloud/.cache/torch/hub/ultralytics_yolov5_master\n",
      "11/15/2023 11:40:36 - INFO - numexpr.utils -   Note: NumExpr detected 10 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "11/15/2023 11:40:36 - INFO - numexpr.utils -   NumExpr defaulting to 8 threads.\n",
      "YOLOv5 🚀 2023-10-24 Python-3.8.10 torch-2.0.1+cu117 CUDA:0 (Tesla T4, 14927MiB)\n",
      "\n",
      "Fusing layers... \n",
      "custom5m summary: 212 layers, 21300138 parameters, 0 gradients, 49.3 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "## Hyperparameters\n",
    "skip = 0 # Number of batches to skip (used to resume script from a specific batch)\n",
    "batch_size = 8 # Batch size for chunked loading of images\n",
    "n_batches = 8 # Number of batches to load\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype=torch.float16\n",
    "real_class_index = 0 # The real class index of the species to fine-tune on (6 for the initial model; insectGBIF-1280m6.pt)\n",
    "inference_size = 1280 # The size of the images to run inference on (1280 for the initial model; insectGBIF-1280m6.pt)\n",
    "image_directory = \"/home/ucloud/testCrops/Set5/\"\n",
    "\n",
    "model_weights = \"insect_iter7-1280m6.pt\"\n",
    "model = torch.hub.load(\n",
    "    'ultralytics/yolov5', \n",
    "    'custom', \n",
    "    path=f'models/{model_weights}',\n",
    "    force_reload=False)\n",
    "\n",
    "model = model.to(device,dtype=dtype)\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sahi.predict import get_sliced_prediction\n",
    "from sahi import AutoDetectionModel\n",
    "\n",
    "detection_model = AutoDetectionModel.from_pretrained(\n",
    "    model_type='yolov5',\n",
    "    model_path=\"models/\" + model_weights,\n",
    "    config_path=\"models/custom5m.yaml\",\n",
    "    confidence_threshold=0.25,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [image_directory + os.sep + i for i in os.listdir(image_directory) if i.endswith(\".jpg\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(path, size=inference_size):\n",
    "    image = read_image(path, mode=torchvision.io.image.ImageReadMode.RGB)\n",
    "    if size is not None:\n",
    "        image = torch.functional.F.interpolate(image.unsqueeze(0), size=size, mode=\"bilinear\")\n",
    "    else:\n",
    "        image = image.unsqueeze(0)\n",
    "    image = image.to(device,dtype=dtype)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice_sizes = np.arange(256, 512 + 1, 64).tolist()\n",
    "# sahi_output = []\n",
    "\n",
    "# for which_image in range(3):\n",
    "\n",
    "#     these_predictions = [\n",
    "#         get_sliced_prediction(\n",
    "#         images[which_image],\n",
    "#         detection_model,\n",
    "#         slice_height=i,\n",
    "#         slice_width=i,\n",
    "#         perform_standard_pred=False,\n",
    "#         postprocess_type=\"GREEDYNMM\",\n",
    "#         postprocess_match_threshold=1,\n",
    "#         overlap_height_ratio=0.25,\n",
    "#         overlap_width_ratio=0.25,\n",
    "#         )\n",
    "#         for i in slice_sizes\n",
    "#     ]\n",
    "\n",
    "#     this_combined_prediction = plot_list(\n",
    "#         these_predictions,\n",
    "#         images[which_image],\n",
    "#         # mult=[1, 19/20, 18/20, 17/20],\n",
    "#         slice_sizes=slice_sizes,\n",
    "#         conf_threshold=0.8,\n",
    "#         match_threshold=0.1,\n",
    "#         match_metric=\"IOU\",\n",
    "#         output_dir=\"sahi/plots\", \n",
    "#         hide_labels=True, \n",
    "#         rect_th=2,\n",
    "#         file_name=f'multi_slice_nms_{which_image}'\n",
    "#     )\n",
    "\n",
    "#     sahi_output += [this_combined_prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "masks = torch.load(\"models/masks.pt\", map_location=device)\n",
    "class_handles = pickle.load(open(\"models/class_handles.pkl\", \"rb\"))\n",
    "model_path = \"models/run27/epoch_0_batch_final.pt\"\n",
    "\n",
    "# Model definition\n",
    "model = torchvision.models.efficientnet_b0(weights=None).half().train(False)\n",
    "num_features = [k for k in [j for j in [i for i in model.children()][0].children()][-1].children()][0].out_channels\n",
    "num_classes = class_handles[\"n_classes\"]\n",
    "\n",
    "class HierarchicalClassifier(nn.Module):\n",
    "    def __init__(self, num_features, num_classes, masks, class_handles=None, _dtype=torch.bfloat16):\n",
    "        super(HierarchicalClassifier, self).__init__()\n",
    "        self.silu = nn.SiLU()\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.bn1 = nn.BatchNorm1d(num_features, device=device, dtype=_dtype)\n",
    "        self.linear1 = nn.Linear(num_features, 1024, device=device, dtype=_dtype)\n",
    "        self.bn2 = nn.BatchNorm1d(1024, device=device, dtype=_dtype)\n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "        self.linear2 = nn.Linear(1024, 1024, device=device, dtype=_dtype)\n",
    "        self.bn3 = nn.BatchNorm1d(1024, device=device, dtype=_dtype)\n",
    "        self.dropout3 = nn.Dropout(0.1)\n",
    "        self.linear3 = nn.Linear(1024, 512, device=device, dtype=_dtype)\n",
    "        self.bn4 = nn.BatchNorm1d(512, device=device, dtype=_dtype)\n",
    "        self.dropout4 = nn.Dropout(0.1)\n",
    "        # self.linear_logits = [nn.Linear(512, ncls, device=device, dtype=dtype) for ncls in num_classes]\n",
    "        self.leaf_logits = nn.Linear(512, num_classes[0], device=device, dtype=_dtype)\n",
    "        self.bn5 = nn.BatchNorm1d(num_classes[0], device=device, dtype=_dtype)\n",
    "        self.masks = masks.copy()\n",
    "        self.class_handles = deepcopy(class_handles)\n",
    "        self.return_embeddings = False\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.return_embeddings:\n",
    "            embeddings = x.clone()\n",
    "        x = self.dropout1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.silu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.silu(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.linear3(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.silu(x)\n",
    "        x = self.dropout4(x)\n",
    "        x = self.leaf_logits(x)\n",
    "        x = self.silu(x)\n",
    "        y = self.bn5(x)\n",
    "        y0 = F.log_softmax(y, dim = 1)\n",
    "        y1 = F.log_softmax(torch.logsumexp(y0.unsqueeze(2) + self.masks[0].T, dim = 1), dim = 1)\n",
    "        y2 = F.log_softmax(torch.logsumexp(y1.unsqueeze(2) + self.masks[1].T, dim = 1), dim = 1)\n",
    "        if self.return_embeddings:\n",
    "            return [y0, y1, y2], embeddings\n",
    "        else:\n",
    "            return [y0, y1, y2]\n",
    "        \n",
    "    def toggle_embeddings(self, value=None):\n",
    "        orig_value = self.return_embeddings\n",
    "        if value:\n",
    "            assert isinstance(value, bool), ValueError(\"Value must be a boolean\")\n",
    "            self.return_embeddings = value\n",
    "        else:\n",
    "            self.return_embeddings = not self.return_embeddings\n",
    "        return orig_value\n",
    "    \n",
    "    def save_state(self, path):\n",
    "        torch.save(self.state_dict(), path)\n",
    "        pickle.dump(self.masks, open(path + \".masks\", \"wb\"))\n",
    "        pickle.dump(self.class_handles, open(path + \".class_handles\", \"wb\"))\n",
    "\n",
    "    def load_state(self, path):\n",
    "        self.load_state_dict(torch.load(path))\n",
    "        self.masks = pickle.load(open(path + \".masks\", \"rb\"))\n",
    "        self.class_handles = pickle.load(open(path + \".class_handles\", \"rb\"))\n",
    "\n",
    "\n",
    "model.to(device=device, dtype=torch.bfloat16)\n",
    "model.classifier = HierarchicalClassifier(num_features, num_classes, masks)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "\n",
    "def augment_float_tensor(x):\n",
    "    tx = x.clone()\n",
    "    tx = tx.permute(1, 2, 0).cpu().numpy() / 255\n",
    "    tx = augment(image=tx)['image']\n",
    "    tx = torch.tensor(tx).permute(2, 0, 1).to(device, dtype=dtype) * 255\n",
    "    return tx\n",
    "\n",
    "def do_define_call_with_augment(model):\n",
    "    old_call = model.__call__\n",
    "    _dtype = model.parameters().__next__().dtype\n",
    "    _device = model.parameters().__next__().device\n",
    "\n",
    "    def call_with_augment(self, x, do_augment=False, n=3, **kwargs):\n",
    "        if do_augment:\n",
    "            xs = []\n",
    "            for tx in x:\n",
    "                xs += [torch.stack([tx.clone()] + [augment_float_tensor(tx.float()).to(_dtype) for _ in range(n)])]\n",
    "            xs = torch.stack(xs)\n",
    "            xs = xs.to(device=_device, dtype=_dtype)\n",
    "            if len(xs.shape) > 4:\n",
    "                augment_preds = [old_call(xsi, **kwargs) for xsi in xs]\n",
    "                if self.classifier.return_embeddings:\n",
    "                    augment_embeddings = torch.stack([ap[1].mean(0) for ap in augment_preds])\n",
    "                    augment_preds = [ap[0] for ap in augment_preds]\n",
    "                combine_pred = [torch.stack([torch.stack([ap[i][j] for j in range(n + 1)]).log_softmax(1) for ap in augment_preds]) for i in range(len(augment_preds[0]))]\n",
    "                single_pred = [torch.stack([(j.float().logsumexp(0) - torch.log(torch.tensor(j.shape[0])).to(j.dtype).to(j.device)).log_softmax(0) for j in combine_pred[i]]) for i in range(len(combine_pred))]\n",
    "            else:\n",
    "                augment_preds = old_call(xs, **kwargs)\n",
    "                if self.classifier.return_embeddings:\n",
    "                    augment_embeddings = augment_preds[1].mean(0)\n",
    "                    augment_preds = augment_preds[0]\n",
    "                combine_pred = [torch.cat([ap[i] for ap in augment_preds]).log_softmax(0) for i in range(len(augment_preds[0]))]\n",
    "                single_pred = [(j.float().logsumexp(0) - torch.log(torch.tensor(j.shape[0])).to(j.dtype).to(j.device)).log_softmax(0) for j in combine_pred]\n",
    "            if self.classifier.return_embeddings:\n",
    "                return single_pred, augment_embeddings\n",
    "            else:\n",
    "                return single_pred\n",
    "        else:\n",
    "            return old_call(x, **kwargs)\n",
    "\n",
    "    model.__call__ = types.MethodType(call_with_augment, model)\n",
    "    return model\n",
    "\n",
    "model = do_define_call_with_augment(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
    "class create_image_preprocessing:\n",
    "    def __init__(self, weights):\n",
    "        self.transform = weights.transforms.func(crop_size=256)\n",
    "        self.isize = self.transform.resize_size[0]\n",
    "        self.mean = self.transform.mean\n",
    "        self.std = self.transform.std\n",
    "\n",
    "    def __call__(self, images):\n",
    "        \"\"\"Preprocess images for EfficientNet.\"\"\"\n",
    "        images = torchvision.transforms.Resize((self.isize, self.isize), antialias=True)(images)\n",
    "        return self.transform(images)\n",
    "\n",
    "image_preprocessing = create_image_preprocessing(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shannon_entropy(t):\n",
    "    \"\"\"\n",
    "    This function computes the row-wise Shannon entropy of a tensor.\n",
    "    \n",
    "    Args:\n",
    "    - t (torch.Tensor): Input tensor of shape (n, m).\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: Row-wise Shannon entropy of shape (n,).\n",
    "    \"\"\"\n",
    "\n",
    "    return (-t * torch.log2(t)).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def bbox_predict(model, output, image, epistemic_threshold=2, background_threshold=0.3, transform=None):\n",
    "    _dtype = model.parameters().__next__().dtype\n",
    "    _device = model.parameters().__next__().device\n",
    "\n",
    "    # Convert SAHI output to to torch.Tensor of shape (n, 4) with xmin, ymin, xmax, ymax for each bounding box\n",
    "    tensor_bboxes = torch.tensor([list(i.bbox.__dict__.values())[:4] for i in output.object_prediction_list]).round().float().to(device=_device)\n",
    "    tensor_image = get_image(image, size=None).float().to(device=_device)\n",
    "\n",
    "    untransformed_inputs = torchvision.ops.roi_align(tensor_image, [tensor_bboxes], output_size=(256, 256))\n",
    "    if transform is not None:\n",
    "        inputs = transform(untransformed_inputs)\n",
    "    inputs = inputs.to(device=_device, dtype=_dtype)\n",
    "\n",
    "    model_uses_embeddings = model.classifier.toggle_embeddings(True)\n",
    "    with torch.no_grad():\n",
    "        crop_pred, crop_embeddings = model.__call__(inputs, do_augment=True, n=25)\n",
    "\n",
    "    max_scores = torch.stack([crop_pred[i].max(1).values for i in range(3)])\n",
    "    score_multiplier = max_scores.T.diff(1).exp()\n",
    "    best_level = [torch.where(i)[0].max().item() + 1 if any(i) else 0 for i in (score_multiplier > epistemic_threshold)]\n",
    "\n",
    "    predicted_class, predicted_class_name, prediction_confidence = [], [], []\n",
    "    for i in range(len(inputs)):\n",
    "        this_scores = crop_pred[best_level[i]][i]\n",
    "        this_predicted_class = int(this_scores.argmax(0).int().cpu().numpy())\n",
    "        this_predicted_class_name = class_handles[\"idx_to_class\"][best_level[i]][this_predicted_class]\n",
    "        this_prediction_confidence = this_scores.max(0).values.exp().float().cpu().numpy()\n",
    "\n",
    "        predicted_class += [this_predicted_class]\n",
    "        predicted_class_name += [this_predicted_class_name] if this_prediction_confidence > background_threshold else [\"background\"]\n",
    "        prediction_confidence += [this_prediction_confidence]\n",
    "\n",
    "    # Sort by class and confidence\n",
    "    order = np.lexsort((prediction_confidence, predicted_class))\n",
    "    untransformed_inputs = untransformed_inputs[order]\n",
    "    predicted_class = [predicted_class[i] for i in order]\n",
    "    predicted_class_name = [predicted_class_name[i] for i in order]\n",
    "    prediction_confidence = [prediction_confidence[i] for i in order]\n",
    "    crop_embeddings = crop_embeddings[order]\n",
    "    best_level = [best_level[i] for i in order]\n",
    "\n",
    "    # Restore model state\n",
    "    model.classifier.toggle_embeddings(model_uses_embeddings)\n",
    "\n",
    "    # Return the cropped tensors, predicted class level, predicted class index, predicted class name, and prediction confidence\n",
    "    return untransformed_inputs, best_level, predicted_class, predicted_class_name, prediction_confidence, crop_embeddings\n",
    "\n",
    "def plot_batch_predict(images, labels, conf, output_file=\"sahi/plots/crops.png\", mixed=True):\n",
    "    if mixed:\n",
    "        images_background = torch.stack([i for i, l in zip(images, labels) if l == \"background\"])\n",
    "        labels_background = [l for l in labels if l == \"background\"]\n",
    "        conf_background = [c for c, l in zip(conf, labels) if l == \"background\"]\n",
    "        plot_batch_predict(images_background, labels_background, conf_background, output_file=output_file.replace(\".png\", \"_background.png\"), mixed=False)\n",
    "\n",
    "        images_salient = torch.stack([i for i, l in zip(images, labels) if l != \"background\"])\n",
    "        labels_salient = [l for l in labels if l != \"background\"]\n",
    "        conf_salient = [c for c, l in zip(conf, labels) if l != \"background\"]\n",
    "        plot_batch_predict(images_salient, labels_salient, conf_salient, output_file=output_file.replace(\".png\", \"_salient.png\"), mixed=False)\n",
    "    else:\n",
    "        # plot crops for debugging\n",
    "        ncol = int(sqrt(len(images)))\n",
    "        nrow = ceil(len(images) / ncol)\n",
    "        assert ncol * nrow >= len(images)\n",
    "\n",
    "        fig, axs = plt.subplots(nrow, ncol, figsize=(ncol * 2, nrow * 2))\n",
    "\n",
    "        for ax, crop, p, s in zip(axs.flatten(), images.int().cpu(), labels, conf):\n",
    "            ax.imshow(crop.cpu().numpy().transpose(1, 2, 0))\n",
    "            p = re.search(r\"^[^_]+(_[^_]+){0,1}\", p).group(0) if p != \"background\" else p\n",
    "            p = p.replace(\"_\", \"\\n\")\n",
    "            ax.set_title(f\"{p}\\n({100 * s:.1f}%)\")\n",
    "            ax.axis(\"off\")\n",
    "        # Set unused axes to invisible\n",
    "        for ax in axs.flatten()[len(images):]:\n",
    "            ax.axis(\"off\")\n",
    "            ax.set_visible(False)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_file, dpi=300)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing image 1 of 20 (SS2 - 20230626030000-167-snapshot.jpg):   0%|          | 0/20 [00:00<?, ?it/s]11/15/2023 16:51:01 - WARNING - sahi.postprocess.combine -   LSNMSPostprocess is experimental and not recommended to use.\n",
      "Processing image 1 of 20 (SS2 - 20230626030000-167-snapshot.jpg):   5%|▌         | 1/20 [00:40<12:57, 40.93s/it]\n"
     ]
    }
   ],
   "source": [
    "slice_sizes = np.arange(256, 512 + 1, 64).tolist()\n",
    "\n",
    "pbar = tqdm(enumerate([images[0]]), total=len(images))\n",
    "\n",
    "for image_idx, this_image in pbar:\n",
    "    pbar.set_description(f\"Processing image {image_idx + 1} of {len(images)} ({this_image.split(os.sep)[-1]})\")\n",
    "    torch.cuda.empty_cache()\n",
    "    # Perform localization on the image\n",
    "    these_predictions = [\n",
    "        get_sliced_prediction(\n",
    "        this_image,\n",
    "        detection_model,\n",
    "        slice_height=i,\n",
    "        slice_width=i,\n",
    "        perform_standard_pred=False,\n",
    "        postprocess_type=\"GREEDYNMM\",\n",
    "        postprocess_match_threshold=1,\n",
    "        overlap_height_ratio=0.25,\n",
    "        overlap_width_ratio=0.25,\n",
    "        verbose=0\n",
    "        )\n",
    "        for i in slice_sizes\n",
    "    ]\n",
    "\n",
    "    # Combine predictions\n",
    "    this_combined_prediction = combine_and_nms(these_predictions, None, slice_sizes, 0.8, \"IOU\", 0.1)\n",
    "\n",
    "    # Perform bounding box classification\n",
    "    crops, plevel, pclass, pclass_name, pconfidence, embeddings = bbox_predict(model, this_combined_prediction, this_image, background_threshold=0.2, transform=lambda x : image_preprocessing(x / 255.0))\n",
    "\n",
    "    # Plot the crops\n",
    "    this_image_name = this_image.split(os.sep)[-1].split(\".\")[0]\n",
    "    plot_batch_predict(crops, pclass_name, pconfidence, output_file=f\"sahi/inference/{this_image_name}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "plt.close()\n",
    "\n",
    "fig, axs = plt.subplots(6, 6, figsize=(24, 24))\n",
    "\n",
    "for i, (ax, crop, p, s) in enumerate(zip(axs.flatten(), crops[:36].int().cpu(), pclass_name, pconfidence)):\n",
    "    ax.imshow(crop.cpu().numpy().transpose(1, 2, 0))\n",
    "    p = re.search(r\"^[^_]+(_[^_]+){0,1}\", p).group(0) if s > 0.3 else \"background\"\n",
    "    p = p.replace(\"_\", \"\\n\")\n",
    "    ax.set_title(f\"{p}\\n({100 * s:.1f}%) - {i}\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"sahi/inference/crops.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 566,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_do_augment = model.classifier.toggle_embeddings(True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_crop = crops[5].clone()\n",
    "\n",
    "    test_crop = test_crop / 255.0\n",
    "    test_crop = image_preprocessing(test_crop)\n",
    "    tout, temb = model.__call__(test_crop.unsqueeze(0), do_augment=True, n = 100)\n",
    "\n",
    "model.classifier.toggle_embeddings(orig_do_augment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 572,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_do_augment = model.classifier.toggle_embeddings(True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_crop = crops[:10].clone()\n",
    "\n",
    "    test_crop = test_crop / 255.0\n",
    "    test_crop = image_preprocessing(test_crop)\n",
    "    tout, temb = model.__call__(test_crop, do_augment=True, n = 25)\n",
    "    \n",
    "model.classifier.toggle_embeddings(orig_do_augment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(tout[0].exp().float().cpu().numpy(), interpolation=\"nearest\", cmap=\"magma\", norm=mpl.colors.LogNorm())\n",
    "plt.gca().set_aspect(\"auto\")\n",
    "\n",
    "plt.colorbar()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"sahi/plots/test1.png\", dpi=300)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(*torch.pca_lowrank(tout[0].float(), 2)[0].cpu().numpy().transpose())\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"sahi/plots/test2.png\", dpi=300)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tiliacea_citrago_Linnaeus_1758 (7.7%)',\n",
       " 'Thaumetopoea_processionea_Linnaeus_1758 (9.0%)',\n",
       " 'Thaumetopoea_processionea_Linnaeus_1758 (23.6%)',\n",
       " 'Scotopteryx_coelinaria_Graslin_1863 (19.2%)',\n",
       " 'Scotopteryx_coelinaria_Graslin_1863 (11.5%)',\n",
       " 'Scotopteryx_coelinaria_Graslin_1863 (15.4%)',\n",
       " 'Thaumetopoea_processionea_Linnaeus_1758 (15.0%)',\n",
       " 'Scotopteryx_coelinaria_Graslin_1863 (15.4%)',\n",
       " 'Scotopteryx_coelinaria_Graslin_1863 (14.6%)',\n",
       " 'Scotopteryx_coelinaria_Graslin_1863 (19.2%)']"
      ]
     },
     "execution_count": 575,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[f'{class_handles[\"idx_to_class\"][0][i.argmax(0).int().item()]} ({i.max().exp().item() * 100:.1f}%)' for i in tout[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crop_ord = torch.pca_lowrank(test_crop_inf[1].float(), 2)[0]\n",
    "# Try umap for testing\n",
    "import umap\n",
    "import matplotlib as mpl\n",
    "crop_ord = umap.UMAP(n_components=2, metric = \"euclidean\", min_dist = 0.05, n_neighbors = 15).fit_transform(embeddings.float().cpu())\n",
    "crop_ord = torch.tensor(crop_ord)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(*crop_ord.cpu().T)\n",
    "plt.savefig(\"sahi/plots/crop_pca.png\", dpi=300)\n",
    "\n",
    "# Plot each crop at the PCA coordinates, in a sort of \"scatterplot\" of the crops\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "\n",
    "crop_ord_screen_coords = crop_ord.clone()\n",
    "crop_ord_screen_coords -= crop_ord_screen_coords.min(0, keepdim=True).values\n",
    "crop_ord_screen_coords /= crop_ord_screen_coords.max(0, keepdim=True).values\n",
    "\n",
    "plt.figure(figsize=(30, 30))\n",
    "ax = plt.gca()\n",
    "\n",
    "# define colormap for classes\n",
    "cmap = mpl.colormaps['tab20']\n",
    "norm = mpl.colors.Normalize(vmin=0, vmax=len(set(pclass_name)))\n",
    "cmap = mpl.cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "\n",
    "for i, (x, y) in enumerate(crop_ord_screen_coords.cpu()):\n",
    "    image = crops[i].int().cpu().numpy().transpose(1, 2, 0)\n",
    "    conf = pconfidence[i].item()  # Prediction confidence\n",
    "\n",
    "    # Adjust the alpha of the image based on the confidence\n",
    "    im = OffsetImage(image, zoom=0.25, alpha=1)  # Adjust zoom as needed\n",
    "    box_color = list(cmap.to_rgba(pclass[i])) if pclass_name[i] != \"background\" else [0, 0, 0, 0]\n",
    "    box_color[3] = conf  # Adjust alpha based on confidence\n",
    "\n",
    "    # Create an annotation box with adjusted alpha for the border\n",
    "    bboxprops = dict(edgecolor=box_color, linewidth=3)  # Add confidence to the RGBA tuple\n",
    "    ab = AnnotationBbox(im, (x, y), xycoords='data', frameon=True, pad=0.1, boxcoords=\"offset points\", bboxprops=bboxprops)\n",
    "    ax.add_artist(ab)\n",
    "\n",
    "    # Extract and format the box label\n",
    "    box_label = re.search(r\"^[^_]+(_[^_]+){0,1}\", pclass_name[i]).group(0)\n",
    "    box_label = box_label.replace(\"_\", \"\\n\")\n",
    "\n",
    "    # Adjust these parameters to position the annotation above the box\n",
    "    text_offset = 20  # Adjust this value as needed\n",
    "    # Adjust the alpha of the text and its background based on confidence\n",
    "    if pclass_name[i] != \"background\":\n",
    "        ax.annotate(box_label, (x, y), xycoords='data', xytext=(0, text_offset), textcoords='offset points', ha=\"center\", va=\"bottom\", fontsize=12, color=\"white\", bbox=dict(boxstyle=\"round\", fc=box_color, ec=\"none\", alpha=conf))\n",
    "\n",
    "plt.savefig(\"sahi/plots/crop_pca_scatter.png\", dpi=300)\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
